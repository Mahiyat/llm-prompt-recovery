{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67121,"databundleVersionId":7806901,"sourceType":"competition"},{"sourceId":7715453,"sourceType":"datasetVersion","datasetId":4505960},{"sourceId":7715470,"sourceType":"datasetVersion","datasetId":4505971},{"sourceId":11358,"sourceType":"modelInstanceVersion","modelInstanceId":5383},{"sourceId":11359,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":8749}],"dockerImageVersionId":30665,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\ntrain_df = pd.read_csv('/kaggle/input/llm-prompt-recovery/train.csv')\n# train_df.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-16T09:13:59.187375Z","iopub.execute_input":"2024-04-16T09:13:59.188215Z","iopub.status.idle":"2024-04-16T09:13:59.549424Z","shell.execute_reply.started":"2024-04-16T09:13:59.188179Z","shell.execute_reply":"2024-04-16T09:13:59.548589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df.loc[0,'original_text']","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:13:59.551673Z","iopub.execute_input":"2024-04-16T09:13:59.552004Z","iopub.status.idle":"2024-04-16T09:13:59.556342Z","shell.execute_reply.started":"2024-04-16T09:13:59.551972Z","shell.execute_reply":"2024-04-16T09:13:59.555364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df.loc[0,'rewrite_prompt']","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:13:59.557780Z","iopub.execute_input":"2024-04-16T09:13:59.558252Z","iopub.status.idle":"2024-04-16T09:13:59.565549Z","shell.execute_reply.started":"2024-04-16T09:13:59.558226Z","shell.execute_reply":"2024-04-16T09:13:59.564683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df.loc[0,'rewritten_text']","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:13:59.566563Z","iopub.execute_input":"2024-04-16T09:13:59.566839Z","iopub.status.idle":"2024-04-16T09:13:59.575672Z","shell.execute_reply.started":"2024-04-16T09:13:59.566816Z","shell.execute_reply":"2024-04-16T09:13:59.574847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/gemma/\n!cp /kaggle/input/gemma-pytorch/gemma_pytorch-main/gemma/* /kaggle/working/gemma/","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:13:59.577890Z","iopub.execute_input":"2024-04-16T09:13:59.578199Z","iopub.status.idle":"2024-04-16T09:14:01.511244Z","shell.execute_reply.started":"2024-04-16T09:13:59.578175Z","shell.execute_reply":"2024-04-16T09:14:01.509929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --no-index --no-deps /kaggle/input/immutabledict/immutabledict-4.1.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:14:01.512938Z","iopub.execute_input":"2024-04-16T09:14:01.513818Z","iopub.status.idle":"2024-04-16T09:14:03.878028Z","shell.execute_reply.started":"2024-04-16T09:14:01.513778Z","shell.execute_reply":"2024-04-16T09:14:03.876850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys \nsys.path.append(\"/kaggle/working/\") \nfrom gemma.config import GemmaConfig, get_config_for_7b, get_config_for_2b\nfrom gemma.model import GemmaForCausalLM\nfrom gemma.tokenizer import Tokenizer\nimport contextlib\nimport os\nimport torch\n# Load the model\nVARIANT = \"2b-it\" \nMACHINE_TYPE = \"cuda\" \nweights_dir = '/kaggle/input/gemma/pytorch/2b-it/2'\n\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n  torch.set_default_dtype(dtype)\n  yield\n  torch.set_default_dtype(torch.float)\n\n# Model Config.\nmodel_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\nmodel_config.tokenizer = os.path.join(weights_dir, \"tokenizer.model\")\nmodel_config.quant = \"quant\" in VARIANT\n\n# Model.\ndevice = torch.device(MACHINE_TYPE)\nwith _set_default_tensor_type(model_config.get_dtype()):\n  model = GemmaForCausalLM(model_config)\n  ckpt_path = os.path.join(weights_dir, f'gemma-{VARIANT}.ckpt')\n  model.load_weights(ckpt_path)\n  model = model.to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:14:03.879505Z","iopub.execute_input":"2024-04-16T09:14:03.879809Z","iopub.status.idle":"2024-04-16T09:14:42.902482Z","shell.execute_reply.started":"2024-04-16T09:14:03.879781Z","shell.execute_reply":"2024-04-16T09:14:42.901426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nrandom.seed(0)\n# This is the prompt format the model expects\nUSER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:14:42.903609Z","iopub.execute_input":"2024-04-16T09:14:42.904024Z","iopub.status.idle":"2024-04-16T09:14:42.908524Z","shell.execute_reply.started":"2024-04-16T09:14:42.903998Z","shell.execute_reply":"2024-04-16T09:14:42.907647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prompt_for_llm = (\n#     \"<start_of_turn>user\\nYou are a smart linguist and you are solving a puzzle.You need to generate a rewrite_prompt that effectively transforms the given original_text into the provided rewritten_text.\"\n#     \"Capture the essence,tone,style,and context of the content while improving the language, coherence, and expressiveness.\"\n#     \"Pay attention to detail, clarity, and overall quality in your generated rewrite_prompt.\"\n#     \"Here is an example sample: original text-\" + train_df.loc[0, 'original_text'] +\n#     \"rewritten_text-\" + train_df.loc[0, 'rewritten_text'] +\n#     \"and this is the right rewrite_prompt-\" + train_df.loc[0, 'rewrite_prompt'] +\n#     \"Now, You will output in text the most suitable rewrite_prompt. For the given original_text- {ot}\" +\n#     \"and rewritten_text- {rt}\" +\n#     \"<end_of_turn>\\n<start_of_turn>model\\n\"\n# )\nprompt_for_llm = (\n    \"<start_of_turn>user\\nYou are a smart and talented linguist who loves to take challenges. You are given to solve a puzzle. You need to generate a rewrite_prompt that effectively transforms the given original_text into the provided rewritten_text.\"\n    \"You need to capture the essence,tone,style,and context of the content while improving the language, coherence, and expressiveness.\"\n    \"Also pay attention to the detail, clarity, and overall quality in your generated rewrite_prompt.\"\n    \"Here is an example sample: original text-\" + train_df.loc[0, 'original_text'] +\n    \"rewritten_text-\" + train_df.loc[0, 'rewritten_text'] +\n    \"and this is the right rewrite_prompt-\" + train_df.loc[0, 'rewrite_prompt'] +\n    \"Now, you will output in text the most suitable rewrite_prompt. For the given original_text- {ot}\" +\n    \"and rewritten_text- {rt}\" +\n    \"<end_of_turn>\\n<start_of_turn>model\\n\"\n)\n# prompt_for_llm = (\n#     \"<start_of_turn>user\\nYou are a smart and talented linguist who loves to take challenges. You are given to solve a puzzle. You need to generate a rewrite_prompt that effectively transforms the given original_text into the provided rewritten_text.\"\n#     \"You need to capture the essence,tone,style,and context of the content while improving the language, coherence, and expressiveness.\"\n#     \"Also pay attention to the detail, clarity, and overall quality in your generated rewrite_prompt.\"\n#     \"Here is an example sample: For the original text-\" + train_df.loc[0, 'original_text'] +\n#     \"and rewritten_text-\" + train_df.loc[0, 'rewritten_text'] +\n#     \",the rightly generated rewrite_prompt-\" + train_df.loc[0, 'rewrite_prompt'] +\n#     \"Now, you will output in text the most suitable rewrite_prompt, for the given original_text- {ot}\" +\n#     \"and rewritten_text- {rt}\" +\n#     \"<end_of_turn>\\n<start_of_turn>model\\n\"\n# )","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:14:42.909786Z","iopub.execute_input":"2024-04-16T09:14:42.910065Z","iopub.status.idle":"2024-04-16T09:14:42.924719Z","shell.execute_reply.started":"2024-04-16T09:14:42.910041Z","shell.execute_reply":"2024-04-16T09:14:42.923749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/llm-prompt-recovery/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:14:42.925915Z","iopub.execute_input":"2024-04-16T09:14:42.926207Z","iopub.status.idle":"2024-04-16T09:14:42.945797Z","shell.execute_reply.started":"2024-04-16T09:14:42.926182Z","shell.execute_reply":"2024-04-16T09:14:42.944971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_sub = pd.read_csv('/kaggle/input/llm-prompt-recovery/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:14:42.946868Z","iopub.execute_input":"2024-04-16T09:14:42.947161Z","iopub.status.idle":"2024-04-16T09:14:42.951478Z","shell.execute_reply.started":"2024-04-16T09:14:42.947136Z","shell.execute_reply":"2024-04-16T09:14:42.950627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:14:42.952856Z","iopub.execute_input":"2024-04-16T09:14:42.953559Z","iopub.status.idle":"2024-04-16T09:14:42.960435Z","shell.execute_reply.started":"2024-04-16T09:14:42.953531Z","shell.execute_reply":"2024-04-16T09:14:42.959518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_sub","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:14:42.961653Z","iopub.execute_input":"2024-04-16T09:14:42.961949Z","iopub.status.idle":"2024-04-16T09:14:42.968991Z","shell.execute_reply.started":"2024-04-16T09:14:42.961926Z","shell.execute_reply":"2024-04-16T09:14:42.968059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nids = []\n\nbatch_size = min(16, len(test))\n\nfor i in range(0, len(test), batch_size):\n    batch_original_texts = test.loc[i:i+batch_size-1, 'original_text'].tolist()\n    batch_rewritten_texts = test.loc[i:i+batch_size-1, 'rewritten_text'].tolist()\n    batch_ids = test.loc[i:i+batch_size-1, 'id'].tolist()\n    \n    batch_predictions = []\n    \n    for original_text, rewritten_text in zip(batch_original_texts, batch_rewritten_texts):\n        rewrite_prompt = model.generate(\n            prompt_for_llm.format(ot=original_text, rt=rewritten_text),\n            device=device,\n            output_len=512,\n        )\n        batch_predictions.append(rewrite_prompt)\n    \n    predictions.extend(batch_predictions)\n    ids.extend(batch_ids)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:14:42.974726Z","iopub.execute_input":"2024-04-16T09:14:42.975374Z","iopub.status.idle":"2024-04-16T09:14:57.268265Z","shell.execute_reply.started":"2024-04-16T09:14:42.975327Z","shell.execute_reply":"2024-04-16T09:14:57.267413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:14:57.269755Z","iopub.execute_input":"2024-04-16T09:14:57.270128Z","iopub.status.idle":"2024-04-16T09:14:57.277311Z","shell.execute_reply.started":"2024-04-16T09:14:57.270077Z","shell.execute_reply":"2024-04-16T09:14:57.276382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict = {'id': ids, 'rewrite_prompt': predictions}\nsample_sub = pd.DataFrame(dict)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:14:57.278624Z","iopub.execute_input":"2024-04-16T09:14:57.278949Z","iopub.status.idle":"2024-04-16T09:14:57.287224Z","shell.execute_reply.started":"2024-04-16T09:14:57.278920Z","shell.execute_reply":"2024-04-16T09:14:57.286365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:14:57.288407Z","iopub.execute_input":"2024-04-16T09:14:57.288662Z","iopub.status.idle":"2024-04-16T09:14:57.305365Z","shell.execute_reply.started":"2024-04-16T09:14:57.288634Z","shell.execute_reply":"2024-04-16T09:14:57.304480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:14:57.306518Z","iopub.execute_input":"2024-04-16T09:14:57.306773Z","iopub.status.idle":"2024-04-16T09:14:57.313615Z","shell.execute_reply.started":"2024-04-16T09:14:57.306751Z","shell.execute_reply":"2024-04-16T09:14:57.312786Z"},"trusted":true},"execution_count":null,"outputs":[]}]}